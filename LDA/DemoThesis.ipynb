{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, u'0.042*nigga + 0.029*shit + 0.025*bitch + 0.020*like + 0.019*just + 0.017*fuck + 0.012*ass + 0.011*know + 0.010*girl + 0.009*need'), (1, u'0.171*video + 0.167*like + 0.013*soprano + 0.011*movi + 0.010*home + 0.009*time + 0.009*scene + 0.009*part + 0.008*full + 0.008*prank'), (2, u'0.080*book + 0.052*seller + 0.046*best + 0.024*amazon + 0.024*hot + 0.018*kindl + 0.018*know + 0.018*just + 0.013*love + 0.011*publish')]\n",
      "SevenEasyWays [(2, 0.99996948603697955)]\n",
      "SumMannShit [(0, 0.99991914895015577)]\n",
      "classi4u [(1, 0.9996323159510746)]\n",
      "UnkleAce [(1, 0.99994789118054372)]\n",
      "0:01:01.928043\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim import corpora, models\n",
    "from collections import defaultdict\n",
    "from pprint import pprint\n",
    "from datetime import datetime\n",
    "from textblob import TextBlob\n",
    "\n",
    "import gensim\n",
    "import os\n",
    "import re\n",
    "#author: https://rstudio-pubs-static.s3.amazonaws.com/79360_850b2a69980c4488b1db95987a24867a.html\n",
    "#modified: ranjanmanish\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "dictionary = []\n",
    "dictFile = open(\"dict.txt\")\n",
    "for word in dictFile:\n",
    "    word = word.strip()\n",
    "    dictionary.append(word.lower())\n",
    "\n",
    "negList = open(\"negative-words.txt\")\n",
    "\n",
    "for negWord in negList:\n",
    "    negWord = negWord.strip()\n",
    "    dictionary.append(negWord.lower())\n",
    "\n",
    "foulList = open(\"foulWordList.txt\")\n",
    "\n",
    "for foulWord in foulList:\n",
    "    foulWord = foulWord.split(\":\")[0]\n",
    "    foulWord = foulWord.strip()\n",
    "    dictionary.append(foulWord.lower())\n",
    "\n",
    "# create English stop words list\n",
    "en_stop = get_stop_words('en')\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n",
    "#increasing the stop word list as the twitter data set is noisy\n",
    "en_stop.extend([\"demibestfans2016\", \"u\", \"rt\", \"t\", \"s\", \"updat\", \"channel\", \"de\", \"que\", \"la\", \"en\", \"eurekamag\", \"na\", \"sa\", \"ang\", \"keo\", \"ka\", \"lang\", \"le\", \"je\", \"est\", \"c\", \"pa\", \"j\", \"ik\", \"un\", \"et\", \"il\", \"wt\", \"fpjb\", \"fnfjb\", \"rbjb\", \"amp\", \"ini\", \"ada\", \"amant\", \"pushawardskathniel\", \"kathniel\", \"00\", \"05\", \"04\", \"15\", \"16\", \"14\", \"18\", \"aku\", \"niond\", \"da\", \"ich\", \"ero\", \"rtandfollow\", \"da\", \"ich\", \"und\", \"ist\", \"ero\", \"m\", \"da\", \"com\", \"em\", \"um\", \"meu\", \"na\", \"pra\", \"weather\", \"properti\", \"googl\", \"0mm\", \"co\", \"thttps\", \"https\", \"http\", \"n\", \"t\", \"u\", \"for\", \"us\", \"is:\", \"it.\", \"on\", \"i'll\", \"also\", \"of\", \"via\", \"follow\", \"mali\", \"rt\", \"got\", \"nowplay\", \"periscop\", \"stat\", \"replay\", \"katch\", \"biztip\", \"via\", \"radio\", \"commerci\", \"na\", \"sa\", \"ang\", \"ko\", \"ng\", \"mo\", \"aka\", \"ka\", \"ve\", \"ke\", \"kixmi\", \"capricorn\", \"tarnat\", \"today\", \"sagittariu\", \"tauru\", \"votingdevonne23\", \"ff\", \"0\", \"new\", \"go\", \"mm\", \"aku\", \"yang\", \"yg\", \"ni\", \"tak\", \"ada\", \"nak\", \"ya\", \"dutchschultz\", \"strictlybid\", \"lovat\", \"iheartaward\", \"bestfanarmi\", \"que\", \"la\", \"el\", \"en\", \"y\", \"lo\", \"es\", \"ain\", \"wit\", \"votingdevonne23\", \"giveyourheartdd\", \"amant\", \"ootd\", \"bandana\", \"updat\", \"get\", \"channel\", \"pushawardskathniel\", \"kathniel\", \"cbb\", \"uri\", \"santiago\", \"ain'\", 'ain', \"por\", \"para\", \"una\", \"der\", \"ein\", \"aja\", \"kamu\", \"sama\", \"untuk\", \"lagi\", \"ako\", \"kau\", \"dah\", \"dia\", \"kalu\", \"lah\", \"bilai\", \"apa\", \"lagi\", \"pushawardskathniels\", \"hahaha\", \"haha\", \"hahahaha\", \"bestfanarmy\", \"can\", \"don\"])\n",
    "\n",
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()\n",
    "    \n",
    "# create sample documents\n",
    "doc_set=[]\n",
    "\n",
    "# get the current path\n",
    "path = os.getcwd()\n",
    "# bring the path to speed\n",
    "#path = path + \"/ALLIN\"\n",
    "#path = path + \"/NewDATAForExp/CSVs_NF_renamed\"\n",
    "#path = path + \"/FILTERED_ALL\"\n",
    "path = path + \"/TEMP\"\n",
    "\n",
    "#get the list of files\n",
    "lst = os.listdir(path)\n",
    "nameList = []\n",
    "# Now take the files one by one and create one doc per user and add to list\n",
    "startTime = datetime.now()\n",
    "\n",
    "for fileName in lst:\n",
    "    name = fileName.split(\".\")[0]\n",
    "    nameList.append(name)\n",
    "    temp = \"\"\n",
    "    fileadd = path + \"/\"+fileName\n",
    "    fileContent = open(fileadd)\n",
    "    for line in fileContent:\n",
    "        try:\n",
    "            line = line.split(\",\")[2]\n",
    "            # get rid of urls\n",
    "            line = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',line)\n",
    "            #to replace #words to just words \n",
    "            line = re.sub(r'#([^\\s]+)', r'\\1', line)\n",
    "            #line = re.sub(r'#([^\\s]+)', '', line)\n",
    "            # userNames - no use- get rid\n",
    "            line = re.sub('@[^\\s]+','', line)\n",
    "            # new lines were creating issues\n",
    "            line = line.strip('\\'\"')\n",
    "            # throwing exception because of unicode error - Fixed that\n",
    "            line = unicode(line, \"utf8\")\n",
    "            # prepare string\n",
    "            temp = temp + str(line)\n",
    "            #print temp\n",
    "        except:\n",
    "            pass\n",
    "    doc_set.append(temp)\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_set:\n",
    "    filtered_tokens = []\n",
    "    frequency = defaultdict(int)\n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    # remove stop words from tokens\n",
    "    \n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "\n",
    "    \n",
    "    dict_tokens = [i for i in stopped_tokens if  i in dictionary]\n",
    "    \n",
    "    # stopwords from nltk package as well although there is not much difference \n",
    "    # dictionary approach is killing us time uwise\n",
    "    #stopped_tokens = [i for i in tokens if not i in cachedStopWords]\n",
    "    \n",
    "    # stem tokens\n",
    "    # stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "    #stemmed_tokens = [p_stemmer.stem(i) for i in dict_tokens]\n",
    "\n",
    "    for element in dict_tokens:\n",
    "        if len(element) > 2:\n",
    "            filtered_tokens.append(element)\n",
    "    \n",
    "\n",
    "    # how about trying stemming once all the two length words are out \n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in filtered_tokens]\n",
    "\n",
    "    # now going to give a attempt try to correct the words - removing the stemmer for now problem: # spell correction gave us 2 min / document not a feasible option hence\n",
    "    spellcorrected_tokens = []\n",
    "    '''for element in stemmed_tokens:\n",
    "        b = TextBlob(element)\n",
    "        word = b.correct()\n",
    "        spellcorrected_tokens.append(str(word))\n",
    "    '''\n",
    "    texts.append(stemmed_tokens)\n",
    "    #texts.append(filtered_tokens)\n",
    "    #texts.append(spellcorrected_tokens)\n",
    "\n",
    "#if word has appeared just once in corpus - throw that out \n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 3] for text in texts]\n",
    "\n",
    "\n",
    "# turn our tokenized documents into a id <-> term dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "\n",
    "#print type(dictionary)\n",
    "\n",
    "#print dictionary\n",
    "\n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "# generate LDA model\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=3, id2word = dictionary, passes=30)\n",
    "\n",
    "print(ldamodel.print_topics(num_topics=3, num_words = 10))\n",
    "\n",
    "ldamodel.save('./data/lda_twitter_6310user_100topics.model')\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for item  in corpus:\n",
    "    '''print \"*************************************************************\"\n",
    "    print item'''\n",
    "    print nameList[counter],\n",
    "    counter = counter + 1\n",
    "    print ldamodel[item]\n",
    "\n",
    "print datetime.now() - startTime\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SumMannShit [(0, 0.99991922702412461)]\n",
      "MackNSweetJones [(0, 0.83509695305656095), (1, 0.1375334382882715), (2, 0.027369608655167386)]\n",
      "ParsonsBrett2 [(0, 0.5688604724390699), (1, 0.048391132887371617), (2, 0.38274839467355848)]\n"
     ]
    }
   ],
   "source": [
    "path = os.getcwd()\n",
    "path = path + \"/TEST\"\n",
    "\n",
    "doc_test= []\n",
    "#get the list of files\n",
    "lst = os.listdir(path)\n",
    "nameList = []\n",
    "# Now take the files one by one and create one doc per user and add to list\n",
    "startTime = datetime.now()\n",
    "\n",
    "for fileName in lst:\n",
    "    name = fileName.split(\".\")[0]\n",
    "    nameList.append(name)\n",
    "    temp = \"\"\n",
    "    fileadd = path + \"/\"+fileName\n",
    "    fileContent = open(fileadd)\n",
    "    for line in fileContent:\n",
    "        try:\n",
    "            line = line.split(\",\")[2]\n",
    "            # get rid of urls\n",
    "            line = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))','',line)\n",
    "            #to replace #words to just words \n",
    "            line = re.sub(r'#([^\\s]+)', r'\\1', line)\n",
    "            #line = re.sub(r'#([^\\s]+)', '', line)\n",
    "            # userNames - no use- get rid\n",
    "            line = re.sub('@[^\\s]+','', line)\n",
    "            # new lines were creating issues\n",
    "            line = line.strip('\\'\"')\n",
    "            # throwing exception because of unicode error - Fixed that\n",
    "            line = unicode(line, \"utf8\")\n",
    "            # prepare string\n",
    "            temp = temp + str(line)\n",
    "            #print temp\n",
    "        except:\n",
    "            pass\n",
    "    doc_test.append(temp)\n",
    "\n",
    "# list for tokenized documents in loop\n",
    "texts = []\n",
    "\n",
    "# loop through document list\n",
    "for i in doc_test:\n",
    "    filtered_tokens = []\n",
    "    frequency = defaultdict(int)\n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "    # remove stop words from tokens\n",
    "    \n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "\n",
    "   \n",
    "    for element in stopped_tokens:\n",
    "        if len(element) > 2:\n",
    "            filtered_tokens.append(element)\n",
    "    \n",
    "\n",
    "    # how about trying stemming once all the two length words are out \n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in filtered_tokens]\n",
    "\n",
    "    texts.append(stemmed_tokens)\n",
    "    \n",
    "\n",
    "#if word has appeared just once in corpus - throw that out \n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "\n",
    "texts = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "\n",
    "\n",
    "# convert tokenized documents into a document-term matrix\n",
    "corpus_test = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for item  in corpus_test:\n",
    "    print nameList[counter],\n",
    "    print ldamodel[item]\n",
    "    counter = counter +1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
